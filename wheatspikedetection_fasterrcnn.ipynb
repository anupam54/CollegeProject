{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/global-wheat-detection/train/00ea5e5ee.jpg\", width=500)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:36:48.296874Z","iopub.execute_input":"2021-12-07T08:36:48.297205Z","iopub.status.idle":"2021-12-07T08:36:48.324867Z","shell.execute_reply.started":"2021-12-07T08:36:48.297131Z","shell.execute_reply":"2021-12-07T08:36:48.324123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\n\nimport torch\n\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom matplotlib import pyplot as plt\n# Set default figure size\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:36:48.328174Z","iopub.execute_input":"2021-12-07T08:36:48.328502Z","iopub.status.idle":"2021-12-07T08:36:49.628305Z","shell.execute_reply.started":"2021-12-07T08:36:48.328472Z","shell.execute_reply":"2021-12-07T08:36:49.627485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define File Path Constants\nINPUT_DIR = os.path.abspath('/kaggle/input/global-wheat-detection')\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nprint(INPUT_DIR)\nprint(TRAIN_DIR)\n\n# Load and Show Training Labels\nDATA_SET = pd.read_csv(os.path.join(INPUT_DIR, \"train.csv\"))\nprint(DATA_SET)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:36:49.629769Z","iopub.execute_input":"2021-12-07T08:36:49.630128Z","iopub.status.idle":"2021-12-07T08:36:49.870137Z","shell.execute_reply.started":"2021-12-07T08:36:49.63009Z","shell.execute_reply":"2021-12-07T08:36:49.869229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convenience functions for loading images\ndef read_image_from_path(image_path):\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\ndef read_image_from_train_folder(image_id):\n    path = os.path.join(TRAIN_DIR, image_id + \".jpg\")\n    return read_image_from_path(path)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:36:49.87186Z","iopub.execute_input":"2021-12-07T08:36:49.872194Z","iopub.status.idle":"2021-12-07T08:36:49.878116Z","shell.execute_reply.started":"2021-12-07T08:36:49.872157Z","shell.execute_reply":"2021-12-07T08:36:49.877163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test loader functions\nsample_image_id = \"b6ab77fd7\"\nplt.imshow(read_image_from_train_folder(sample_image_id))\nplt.title(sample_image_id)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:36:49.880243Z","iopub.execute_input":"2021-12-07T08:36:49.880964Z","iopub.status.idle":"2021-12-07T08:36:50.412143Z","shell.execute_reply.started":"2021-12-07T08:36:49.880857Z","shell.execute_reply":"2021-12-07T08:36:50.41134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions for parsing bounding box string into x1, y1, x2, y2\ndef parse_bbox_text(string_input):\n    input_without_brackets = re.sub(\"\\[|\\]\", \"\", string_input)\n    input_as_list = np.array(input_without_brackets.split(\",\"))\n    return input_as_list.astype(np.float) \n\ndef xywh_to_x1y1x2y2(x,y,w,h):\n    return np.array([x,y,x+w,y+h])","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:36:50.413326Z","iopub.execute_input":"2021-12-07T08:36:50.413808Z","iopub.status.idle":"2021-12-07T08:36:50.420133Z","shell.execute_reply.started":"2021-12-07T08:36:50.413766Z","shell.execute_reply":"2021-12-07T08:36:50.419307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parse training bounding box labels\ntrain_df = pd.read_csv(os.path.join(INPUT_DIR, \"train.csv\"))\nbbox_series = train_df.bbox.apply(parse_bbox_text)\n\nprint(bbox_series)\nxywh_df = pd.DataFrame(bbox_series.to_list(), columns=[\"x\", \"y\", \"w\", \"h\"])\nprint(xywh_df)\n\nx2_df = pd.DataFrame(xywh_df.x + xywh_df.w, columns=[\"x2\"])\ny2_df = pd.DataFrame(xywh_df.y + xywh_df.h, columns=[\"y2\"])\n\n# Update training dataframe with parsed labels\ntrain_df = train_df.join([xywh_df, x2_df, y2_df])\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:36:50.421396Z","iopub.execute_input":"2021-12-07T08:36:50.421865Z","iopub.status.idle":"2021-12-07T08:36:52.689193Z","shell.execute_reply.started":"2021-12-07T08:36:50.42183Z","shell.execute_reply":"2021-12-07T08:36:52.688213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convenience function for drawing a list of bounding box coordinates on and image\ndef draw_boxes_on_image(boxes, image, color=(255,0,0)):    \n    #print(boxes)\n    for box in boxes:\n        cv2.rectangle(image,\n                      (int(box[0]), int(box[1]) ),\n                      (int(box[2]), int(box[3]) ),\n                      color, 3)\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:36:52.690641Z","iopub.execute_input":"2021-12-07T08:36:52.691064Z","iopub.status.idle":"2021-12-07T08:36:52.697248Z","shell.execute_reply.started":"2021-12-07T08:36:52.691017Z","shell.execute_reply":"2021-12-07T08:36:52.696109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample a random training instance and draw the labelled bounding boxes\nsample_image_id =  train_df.image_id.sample().item()\nprint(train_df.image_id.sample())\nprint(sample_image_id)\n\nsample_image = read_image_from_train_folder(sample_image_id)\n#print(sample_image)\nplt.imshow(sample_image)\nsample_bounding_boxes = train_df[train_df.image_id == sample_image_id][[\"x\", \"y\",\"x2\",\"y2\"]]\nprint(sample_bounding_boxes)\n\nplt.imshow(draw_boxes_on_image(sample_bounding_boxes.to_numpy(), sample_image, color=(0,200,200)))\n_ = plt.title(sample_image_id)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:36:52.70049Z","iopub.execute_input":"2021-12-07T08:36:52.701132Z","iopub.status.idle":"2021-12-07T08:36:53.3373Z","shell.execute_reply.started":"2021-12-07T08:36:52.70109Z","shell.execute_reply":"2021-12-07T08:36:53.335736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download a pre-trained bounding box detector\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:36:53.339057Z","iopub.execute_input":"2021-12-07T08:36:53.339542Z","iopub.status.idle":"2021-12-07T08:37:05.30262Z","shell.execute_reply.started":"2021-12-07T08:36:53.339507Z","shell.execute_reply":"2021-12-07T08:37:05.301705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:37:05.304142Z","iopub.execute_input":"2021-12-07T08:37:05.304716Z","iopub.status.idle":"2021-12-07T08:37:05.321108Z","shell.execute_reply.started":"2021-12-07T08:37:05.304676Z","shell.execute_reply":"2021-12-07T08:37:05.319173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace the pre-trained bounding box detector head with\n# a new one that predicts our desired 2 classes {BACKGROUND, WHEAT}\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nprint(model.roi_heads)\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_channels=in_features, num_classes=2)\n\n# Verify the model architecture\nmodel.roi_heads","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:37:05.323641Z","iopub.execute_input":"2021-12-07T08:37:05.325816Z","iopub.status.idle":"2021-12-07T08:37:05.342319Z","shell.execute_reply.started":"2021-12-07T08:37:05.325773Z","shell.execute_reply":"2021-12-07T08:37:05.341312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determine device to run on. GPU is highly recommended\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndef move_batch_to_device(images, targets):\n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    return images, targets","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:37:05.343936Z","iopub.execute_input":"2021-12-07T08:37:05.346074Z","iopub.status.idle":"2021-12-07T08:37:05.430074Z","shell.execute_reply.started":"2021-12-07T08:37:05.346035Z","shell.execute_reply":"2021-12-07T08:37:05.428958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split data into training and validation subsets\nunique_image_ids = train_df['image_id'].unique()\nprint(unique_image_ids.shape)\n\nn_validation = int(0.2 * len(unique_image_ids))\nprint(n_validation)\nvalid_ids = unique_image_ids[-n_validation:]\nprint(valid_ids)\ntrain_ids = unique_image_ids[:-n_validation]\nprint(train_ids)\n\nvalidation_df = train_df[train_df['image_id'].isin(valid_ids)]\ntraining_df = train_df[train_df['image_id'].isin(train_ids)]\n\nprint(\"%i training samples\\n%i validation samples\" % (len(training_df.image_id.unique()),len(validation_df.image_id.unique())) )","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:37:05.431929Z","iopub.execute_input":"2021-12-07T08:37:05.433096Z","iopub.status.idle":"2021-12-07T08:37:05.539308Z","shell.execute_reply.started":"2021-12-07T08:37:05.433054Z","shell.execute_reply":"2021-12-07T08:37:05.53699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inherit from pytorch Dataset for convenience\n\nclass WheatDataset(Dataset):\n\n    def __init__(self, dataframe):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        #print(self.image_ids)\n        #print(self.df)\n\n    def __len__(self) -> int:\n        return len(self.image_ids)\n    \n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        image = read_image_from_train_folder(image_id).astype(np.float32)\n        # Scale to [0,1] range expected by the pre-trained model\n        image /= 255.0\n        # Convert the shape from [h,w,c] to [c,h,w] as expected by pytorch\n        image = torch.from_numpy(image).permute(2,0,1)\n        \n        records = self.df[self.df['image_id'] == image_id]\n        #print(records)\n        \n        boxes = records[['x', 'y', 'x2', 'y2']].values\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        n_boxes = boxes.shape[0]\n        #print(n_boxes)\n        \n        # there is only one foreground class, WHEAT\n        labels = torch.ones((n_boxes,), dtype=torch.int64)\n        #print(labels)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        #print(['boxes'])\n        #print(['labels'])\n        \n        return image, target","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:37:05.540479Z","iopub.execute_input":"2021-12-07T08:37:05.540942Z","iopub.status.idle":"2021-12-07T08:37:05.557127Z","shell.execute_reply.started":"2021-12-07T08:37:05.540905Z","shell.execute_reply":"2021-12-07T08:37:05.556273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create pytorch data loaders for training and validation\n\ntrain_dataset = WheatDataset(training_df)\nvalid_dataset = WheatDataset(validation_df)\n\n# A function to bring images with different\n# number of bounding boxes into the same batch\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\nis_training_on_cpu = device == torch.device('cpu')\n#print(is_training_on_cpu)\nbatch_size = 4 if is_training_on_cpu else 16\n#print(batch_size)\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n#print(train_data_loader)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n#print(valid_data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:37:05.561273Z","iopub.execute_input":"2021-12-07T08:37:05.564031Z","iopub.status.idle":"2021-12-07T08:37:05.592616Z","shell.execute_reply.started":"2021-12-07T08:37:05.563951Z","shell.execute_reply":"2021-12-07T08:37:05.591651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the data loader\nbatch_of_images, batch_of_targets = next(iter(train_data_loader))\n\nsample_boxes = batch_of_targets[0]['boxes'].cpu().numpy().astype(np.int32)\nsample_image = batch_of_images[0].permute(1,2,0).cpu().numpy() # convert back from pytorch format\n\nplt.imshow(draw_boxes_on_image(sample_boxes, sample_image, color=(0,200,200)))","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:37:05.594083Z","iopub.execute_input":"2021-12-07T08:37:05.594437Z","iopub.status.idle":"2021-12-07T08:37:10.971275Z","shell.execute_reply.started":"2021-12-07T08:37:05.594399Z","shell.execute_reply":"2021-12-07T08:37:10.968306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up the optimiser\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:37:10.972901Z","iopub.execute_input":"2021-12-07T08:37:10.973425Z","iopub.status.idle":"2021-12-07T08:37:10.980471Z","shell.execute_reply.started":"2021-12-07T08:37:10.973385Z","shell.execute_reply":"2021-12-07T08:37:10.979343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 1 if is_training_on_cpu else 3\n\n# Prepare the model for training\nmodel = model.to(device)\nmodel.train()\n    \nfor epoch in range(num_epochs):\n    print(\"Epoch %i/%i \" % (epoch + 1, num_epochs) )\n    average_loss = 0\n    for batch_id, (images, targets) in enumerate(train_data_loader):\n        # Prepare the batch data\n        images, targets = move_batch_to_device(images, targets)\n\n        # Calculate losses\n        loss_dict = model(images, targets)\n        batch_loss = sum(loss for loss in loss_dict.values()) / len(loss_dict)\n        \n        # loss minimization by refreshing the accumulated optimiser state\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n        \n        # Record stats\n        loss_value = batch_loss.item()\n        average_loss = average_loss + (loss_value - average_loss) / (batch_id + 1)\n        print(\"Mini-batch: %i/%i Loss: %.4f\" % ( batch_id + 1, len(train_data_loader), average_loss), end='\\r')\n        if batch_id % 100 == 0:\n            print(\"Mini-batch: %i/%i Loss: %.4f\" % ( batch_id + 1, len(train_data_loader), average_loss))","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:37:10.981814Z","iopub.execute_input":"2021-12-07T08:37:10.982369Z","iopub.status.idle":"2021-12-07T08:59:31.491794Z","shell.execute_reply.started":"2021-12-07T08:37:10.982296Z","shell.execute_reply":"2021-12-07T08:59:31.490751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(loss_dict)\nprint(\"\\n\")\nprint(batch_loss, \"\\n\")\nprint(loss_value, \"\\n\")\nprint(average_loss,\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:59:31.493632Z","iopub.execute_input":"2021-12-07T08:59:31.494044Z","iopub.status.idle":"2021-12-07T08:59:31.509589Z","shell.execute_reply.started":"2021-12-07T08:59:31.493994Z","shell.execute_reply":"2021-12-07T08:59:31.508714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the model for inference\nmodel.eval()\n\n# Prepare a validation results generator\ndef make_validation_iter():\n    valid_data_iter = iter(valid_data_loader)\n    for images, targets in valid_data_iter:\n        images, targets = move_batch_to_device(images, targets)\n\n        cpu_device = torch.device(\"cpu\")\n        outputs = model(images)\n        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        print(outputs)\n        for image, output, target in zip(images, outputs, targets): \n            predicted_boxes = output['boxes'].cpu().detach().numpy().astype(np.int32)\n            ground_truth_boxes = target['boxes'].cpu().numpy().astype(np.int32)\n            image = image.permute(1,2,0).cpu().numpy()\n            yield image, ground_truth_boxes, predicted_boxes\n\nvalidation_iter = make_validation_iter()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:59:31.511019Z","iopub.execute_input":"2021-12-07T08:59:31.5116Z","iopub.status.idle":"2021-12-07T08:59:31.520927Z","shell.execute_reply.started":"2021-12-07T08:59:31.511563Z","shell.execute_reply":"2021-12-07T08:59:31.520083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(model.eval())\n#print(validation_iter)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:59:31.522373Z","iopub.execute_input":"2021-12-07T08:59:31.522807Z","iopub.status.idle":"2021-12-07T08:59:31.533138Z","shell.execute_reply.started":"2021-12-07T08:59:31.522752Z","shell.execute_reply":"2021-12-07T08:59:31.531916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(next(validation_iter))\nimage, ground_truth_boxes, predicted_boxes = next(validation_iter)\nimage = draw_boxes_on_image(predicted_boxes, image, (255,0,0))\nimage = draw_boxes_on_image(ground_truth_boxes, image , (0,255,0))\n#print(predicted_boxes)\n\nplt.imshow(image)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:59:31.534716Z","iopub.execute_input":"2021-12-07T08:59:31.535661Z","iopub.status.idle":"2021-12-07T08:59:36.266384Z","shell.execute_reply.started":"2021-12-07T08:59:31.535615Z","shell.execute_reply":"2021-12-07T08:59:36.265027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(predicted_boxes, ground_truth_boxes)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:59:36.272165Z","iopub.execute_input":"2021-12-07T08:59:36.279068Z","iopub.status.idle":"2021-12-07T08:59:37.187519Z","shell.execute_reply.started":"2021-12-07T08:59:36.279005Z","shell.execute_reply":"2021-12-07T08:59:37.185765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\njoblib.dump(model,'model.pk1')\nmodel = joblib.load('model.pk1')\nimage, ground_truth_boxes, predicted_boxes = next(validation_iter)\nimage = draw_boxes_on_image(predicted_boxes, image, (255,255,0))\nimage = draw_boxes_on_image(ground_truth_boxes, image , (0,255,255))\nplt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:59:37.188913Z","iopub.status.idle":"2021-12-07T08:59:37.189686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'fasterrcnn_gwd_finetuned.pth')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T08:59:37.190992Z","iopub.status.idle":"2021-12-07T08:59:37.191814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}