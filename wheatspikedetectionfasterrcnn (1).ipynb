{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/global-wheat-detection/train/00ea5e5ee.jpg\", width=500)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:01.459702Z","iopub.execute_input":"2021-12-23T08:02:01.460068Z","iopub.status.idle":"2021-12-23T08:02:01.500140Z","shell.execute_reply.started":"2021-12-23T08:02:01.459988Z","shell.execute_reply":"2021-12-23T08:02:01.499399Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\n\nimport torch\n\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom matplotlib import pyplot as plt\n# Set default figure size\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:01.501473Z","iopub.execute_input":"2021-12-23T08:02:01.501845Z","iopub.status.idle":"2021-12-23T08:02:02.841745Z","shell.execute_reply.started":"2021-12-23T08:02:01.501803Z","shell.execute_reply":"2021-12-23T08:02:02.840952Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Define File Path Constants\nINPUT_DIR = os.path.abspath('/kaggle/input/global-wheat-detection')\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nprint(INPUT_DIR)\nprint(TRAIN_DIR)\n\n# Load and Show Training Labels\nDATA_SET = pd.read_csv(os.path.join(INPUT_DIR, \"train.csv\"))\nprint(DATA_SET)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:02.843630Z","iopub.execute_input":"2021-12-23T08:02:02.844019Z","iopub.status.idle":"2021-12-23T08:02:03.107314Z","shell.execute_reply.started":"2021-12-23T08:02:02.843978Z","shell.execute_reply":"2021-12-23T08:02:03.106421Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Convenience functions for loading images\ndef read_image_from_path(image_path):\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\ndef read_image_from_train_folder(image_id):\n    path = os.path.join(TRAIN_DIR, image_id + \".jpg\")\n    return read_image_from_path(path)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:03.108907Z","iopub.execute_input":"2021-12-23T08:02:03.109411Z","iopub.status.idle":"2021-12-23T08:02:03.115153Z","shell.execute_reply.started":"2021-12-23T08:02:03.109366Z","shell.execute_reply":"2021-12-23T08:02:03.114261Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Test loader functions\nsample_image_id = \"b6ab77fd7\"\nplt.imshow(read_image_from_train_folder(sample_image_id))\nplt.title(sample_image_id)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:03.116585Z","iopub.execute_input":"2021-12-23T08:02:03.117119Z","iopub.status.idle":"2021-12-23T08:02:03.632278Z","shell.execute_reply.started":"2021-12-23T08:02:03.117084Z","shell.execute_reply":"2021-12-23T08:02:03.631473Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Functions for parsing bounding box string into x1, y1, x2, y2\ndef parse_bbox_text(string_input):\n    input_without_brackets = re.sub(\"\\[|\\]\", \"\", string_input)\n    input_as_list = np.array(input_without_brackets.split(\",\"))\n    return input_as_list.astype(np.float) \n\ndef xywh_to_x1y1x2y2(x,y,w,h):\n    return np.array([x,y,x+w,y+h])","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:03.633686Z","iopub.execute_input":"2021-12-23T08:02:03.634221Z","iopub.status.idle":"2021-12-23T08:02:03.640619Z","shell.execute_reply.started":"2021-12-23T08:02:03.634181Z","shell.execute_reply":"2021-12-23T08:02:03.639748Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Parse training bounding box labels\ntrain_df = pd.read_csv(os.path.join(INPUT_DIR, \"train.csv\"))\nbbox_series = train_df.bbox.apply(parse_bbox_text)\n\nprint(bbox_series)\nxywh_df = pd.DataFrame(bbox_series.to_list(), columns=[\"x\", \"y\", \"w\", \"h\"])\nprint(xywh_df)\n\nx2_df = pd.DataFrame(xywh_df.x + xywh_df.w, columns=[\"x2\"])\ny2_df = pd.DataFrame(xywh_df.y + xywh_df.h, columns=[\"y2\"])\n\n# Update training dataframe with parsed labels\ntrain_df = train_df.join([xywh_df, x2_df, y2_df])\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:03.642045Z","iopub.execute_input":"2021-12-23T08:02:03.642658Z","iopub.status.idle":"2021-12-23T08:02:05.805704Z","shell.execute_reply.started":"2021-12-23T08:02:03.642619Z","shell.execute_reply":"2021-12-23T08:02:05.804777Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Convenience function for drawing a list of bounding box coordinates on and image\ndef draw_boxes_on_image(boxes, image, color=(255,0,0)):    \n    #print(boxes)\n    for box in boxes:\n        cv2.rectangle(image,\n                      (int(box[0]), int(box[1]) ),\n                      (int(box[2]), int(box[3]) ),\n                      color, 3)\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:05.808724Z","iopub.execute_input":"2021-12-23T08:02:05.809008Z","iopub.status.idle":"2021-12-23T08:02:05.815388Z","shell.execute_reply.started":"2021-12-23T08:02:05.808979Z","shell.execute_reply":"2021-12-23T08:02:05.813364Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Sample a random training instance and draw the labelled bounding boxes\nsample_image_id =  train_df.image_id.sample().item()\nprint(train_df.image_id.sample())\nprint(sample_image_id)\n\nsample_image = read_image_from_train_folder(sample_image_id)\n#print(sample_image)\nplt.imshow(sample_image)\nsample_bounding_boxes = train_df[train_df.image_id == sample_image_id][[\"x\", \"y\",\"x2\",\"y2\"]]\nprint(sample_bounding_boxes)\n\nplt.imshow(draw_boxes_on_image(sample_bounding_boxes.to_numpy(), sample_image, color=(0,200,200)))\n_ = plt.title(sample_image_id)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:05.817976Z","iopub.execute_input":"2021-12-23T08:02:05.818809Z","iopub.status.idle":"2021-12-23T08:02:06.410707Z","shell.execute_reply.started":"2021-12-23T08:02:05.818767Z","shell.execute_reply":"2021-12-23T08:02:06.406784Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Download a pre-trained bounding box detector\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:06.412030Z","iopub.execute_input":"2021-12-23T08:02:06.412726Z","iopub.status.idle":"2021-12-23T08:02:10.811581Z","shell.execute_reply.started":"2021-12-23T08:02:06.412679Z","shell.execute_reply":"2021-12-23T08:02:10.810660Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:10.812870Z","iopub.execute_input":"2021-12-23T08:02:10.813237Z","iopub.status.idle":"2021-12-23T08:02:10.821938Z","shell.execute_reply.started":"2021-12-23T08:02:10.813201Z","shell.execute_reply":"2021-12-23T08:02:10.820972Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Replace the pre-trained bounding box detector head with\n# a new one that predicts our desired 2 classes {BACKGROUND, WHEAT}\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nprint(model.roi_heads)\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_channels=in_features, num_classes=2)\n\n# Verify the model architecture\nmodel.roi_heads","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:10.824565Z","iopub.execute_input":"2021-12-23T08:02:10.825129Z","iopub.status.idle":"2021-12-23T08:02:10.838172Z","shell.execute_reply.started":"2021-12-23T08:02:10.825093Z","shell.execute_reply":"2021-12-23T08:02:10.837390Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Determine device to run on. GPU is highly recommended\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndef move_batch_to_device(images, targets):\n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    return images, targets","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:10.839455Z","iopub.execute_input":"2021-12-23T08:02:10.840026Z","iopub.status.idle":"2021-12-23T08:02:10.904289Z","shell.execute_reply.started":"2021-12-23T08:02:10.839991Z","shell.execute_reply":"2021-12-23T08:02:10.903280Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"## Split data into training and validation subsets\nunique_image_ids = train_df['image_id'].unique()\nprint(unique_image_ids.shape)\n\nn_validation = int(0.2 * len(unique_image_ids))\nprint(n_validation)\nvalid_ids = unique_image_ids[-n_validation:]\nprint(valid_ids)\ntrain_ids = unique_image_ids[:-n_validation]\nprint(train_ids)\n\nvalidation_df = train_df[train_df['image_id'].isin(valid_ids)]\ntraining_df = train_df[train_df['image_id'].isin(train_ids)]\n\nprint(\"%i training samples\\n%i validation samples\" % (len(training_df.image_id.unique()),len(validation_df.image_id.unique())) )","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:10.907495Z","iopub.execute_input":"2021-12-23T08:02:10.909232Z","iopub.status.idle":"2021-12-23T08:02:10.976657Z","shell.execute_reply.started":"2021-12-23T08:02:10.909201Z","shell.execute_reply":"2021-12-23T08:02:10.975805Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Inherit from pytorch Dataset for convenience\n\nclass WheatDataset(Dataset):\n\n    def __init__(self, dataframe):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        #print(self.image_ids)\n        #print(self.df)\n\n    def __len__(self) -> int:\n        return len(self.image_ids)\n    \n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        image = read_image_from_train_folder(image_id).astype(np.float32)\n        # Scale to [0,1] range expected by the pre-trained model\n        image /= 255.0\n        # Convert the shape from [h,w,c] to [c,h,w] as expected by pytorch\n        image = torch.from_numpy(image).permute(2,0,1)\n        \n        records = self.df[self.df['image_id'] == image_id]\n        #print(records)\n        \n        boxes = records[['x', 'y', 'x2', 'y2']].values\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        n_boxes = boxes.shape[0]\n        #print(n_boxes)\n        \n        # there is only one foreground class, WHEAT\n        labels = torch.ones((n_boxes,), dtype=torch.int64)\n        #print(labels)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        #print(['boxes'])\n        #print(['labels'])\n        \n        return image, target","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:10.977825Z","iopub.execute_input":"2021-12-23T08:02:10.978315Z","iopub.status.idle":"2021-12-23T08:02:10.987500Z","shell.execute_reply.started":"2021-12-23T08:02:10.978265Z","shell.execute_reply":"2021-12-23T08:02:10.986613Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Create pytorch data loaders for training and validation\n\ntrain_dataset = WheatDataset(training_df)\nvalid_dataset = WheatDataset(validation_df)\n\n# A function to bring images with different\n# number of bounding boxes into the same batch\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\nis_training_on_cpu = device == torch.device('cpu')\n#print(is_training_on_cpu)\nbatch_size = 4 if is_training_on_cpu else 16\n#print(batch_size)\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n#print(train_data_loader)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n#print(valid_data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:10.988963Z","iopub.execute_input":"2021-12-23T08:02:10.989482Z","iopub.status.idle":"2021-12-23T08:02:11.010497Z","shell.execute_reply.started":"2021-12-23T08:02:10.989432Z","shell.execute_reply":"2021-12-23T08:02:11.009699Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Test the data loader\nbatch_of_images, batch_of_targets = next(iter(train_data_loader))\n\nsample_boxes = batch_of_targets[0]['boxes'].cpu().numpy().astype(np.int32)\nsample_image = batch_of_images[0].permute(1,2,0).cpu().numpy() # convert back from pytorch format\n\nplt.imshow(draw_boxes_on_image(sample_boxes, sample_image, color=(0,200,200)))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:11.013965Z","iopub.execute_input":"2021-12-23T08:02:11.014422Z","iopub.status.idle":"2021-12-23T08:02:16.224090Z","shell.execute_reply.started":"2021-12-23T08:02:11.014391Z","shell.execute_reply":"2021-12-23T08:02:16.223340Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Set up the optimiser\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:16.225545Z","iopub.execute_input":"2021-12-23T08:02:16.226159Z","iopub.status.idle":"2021-12-23T08:02:16.232928Z","shell.execute_reply.started":"2021-12-23T08:02:16.226120Z","shell.execute_reply":"2021-12-23T08:02:16.232250Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"num_epochs = 1 if is_training_on_cpu else 3\n\n# Prepare the model for training\nmodel = model.to(device)\nmodel.train()\n    \nfor epoch in range(num_epochs):\n    print(\"Epoch %i/%i \" % (epoch + 1, num_epochs) )\n    average_loss = 0\n    for batch_id, (images, targets) in enumerate(train_data_loader):\n        # Prepare the batch data\n        images, targets = move_batch_to_device(images, targets)\n\n        # Calculate losses\n        loss_dict = model(images, targets)\n        batch_loss = sum(loss for loss in loss_dict.values()) / len(loss_dict)\n        \n        # loss minimization by refreshing the accumulated optimiser state\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n        \n        # Record stats\n        loss_value = batch_loss.item()\n        average_loss = average_loss + (loss_value - average_loss) / (batch_id + 1)\n        print(\"Mini-batch: %i/%i Loss: %.4f\" % ( batch_id + 1, len(train_data_loader), average_loss), end='\\r')\n        if batch_id % 100 == 0:\n            print(\"Mini-batch: %i/%i Loss: %.4f\" % ( batch_id + 1, len(train_data_loader), average_loss))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:02:16.234222Z","iopub.execute_input":"2021-12-23T08:02:16.234713Z","iopub.status.idle":"2021-12-23T08:24:29.803216Z","shell.execute_reply.started":"2021-12-23T08:02:16.234677Z","shell.execute_reply":"2021-12-23T08:24:29.802105Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(loss_dict)\nprint(\"\\n\")\nprint(batch_loss, \"\\n\")\nprint(loss_value, \"\\n\")\nprint(average_loss,\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:24:29.805214Z","iopub.execute_input":"2021-12-23T08:24:29.805634Z","iopub.status.idle":"2021-12-23T08:24:29.820983Z","shell.execute_reply.started":"2021-12-23T08:24:29.805589Z","shell.execute_reply":"2021-12-23T08:24:29.819935Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Prepare the model for inference\nmodel.eval()\n\n# Prepare a validation results generator\ndef make_validation_iter():\n    valid_data_iter = iter(valid_data_loader)\n    for images, targets in valid_data_iter:\n        images, targets = move_batch_to_device(images, targets)\n\n        cpu_device = torch.device(\"cpu\")\n        outputs = model(images)\n        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        print(outputs)\n        for image, output, target in zip(images, outputs, targets): \n            predicted_boxes = output['boxes'].cpu().detach().numpy().astype(np.int32)\n            ground_truth_boxes = target['boxes'].cpu().numpy().astype(np.int32)\n            image = image.permute(1,2,0).cpu().numpy()\n            yield image, ground_truth_boxes, predicted_boxes\n\nvalidation_iter = make_validation_iter()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:24:29.822175Z","iopub.execute_input":"2021-12-23T08:24:29.822520Z","iopub.status.idle":"2021-12-23T08:24:29.831349Z","shell.execute_reply.started":"2021-12-23T08:24:29.822485Z","shell.execute_reply":"2021-12-23T08:24:29.830422Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#print(model.eval())\n#print(validation_iter)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:24:29.832619Z","iopub.execute_input":"2021-12-23T08:24:29.833108Z","iopub.status.idle":"2021-12-23T08:24:29.840866Z","shell.execute_reply.started":"2021-12-23T08:24:29.833070Z","shell.execute_reply":"2021-12-23T08:24:29.839969Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#print(next(validation_iter))\nimage, ground_truth_boxes, predicted_boxes = next(validation_iter)\nimage = draw_boxes_on_image(predicted_boxes, image, (255,0,0))\nimage = draw_boxes_on_image(ground_truth_boxes, image , (0,255,0))\n#print(predicted_boxes)\n\nplt.imshow(image)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:24:29.844661Z","iopub.execute_input":"2021-12-23T08:24:29.845052Z","iopub.status.idle":"2021-12-23T08:24:34.490061Z","shell.execute_reply.started":"2021-12-23T08:24:29.845026Z","shell.execute_reply":"2021-12-23T08:24:34.488833Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(predicted_boxes, ground_truth_boxes)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:24:34.492312Z","iopub.execute_input":"2021-12-23T08:24:34.492888Z","iopub.status.idle":"2021-12-23T08:24:35.446795Z","shell.execute_reply.started":"2021-12-23T08:24:34.492828Z","shell.execute_reply":"2021-12-23T08:24:35.445356Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import joblib\njoblib.dump(model,'model.pk1')\nmodel = joblib.load('model.pk1')\nimage, ground_truth_boxes, predicted_boxes = next(validation_iter)\nimage = draw_boxes_on_image(predicted_boxes, image, (255,255,0))\nimage = draw_boxes_on_image(ground_truth_boxes, image , (0,255,255))\nplt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:24:35.447967Z","iopub.status.idle":"2021-12-23T08:24:35.448716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'fasterrcnn_gwd_finetuned.pth')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:24:35.449872Z","iopub.status.idle":"2021-12-23T08:24:35.450578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}